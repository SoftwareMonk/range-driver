{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML output for this notebook can be produced using\n",
    "# jupyter nbconvert --to html --no-input tidal_analysis_rt.ipynb\n",
    "# or\n",
    "# jupyter nbconvert --to pdf --no-input tidal_analysis_rt.ipynb\n",
    "# edit title and authors in notebook metadata (e.g. jupyter lab / notebook tools / advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of acoustic tracking performance using range test data obtained in Mahone Bay near Halifax, NS, by OTN field experiments during March-April 2016.\n",
    "\n",
    "Range test performance is determined with two methods:\n",
    "\n",
    "  1. by calculating interval lenghts between adjacent detection events,\n",
    "  1. by counting the number of detection events within some fixed time interval and normalizing against the expected number of detections.\n",
    "\n",
    "Components of this notebook:\n",
    "\n",
    " * process tidal data for the time period considering high/low tide times and the observed heights\n",
    " * determine tidal phase timing\n",
    " * perform cosine interpolation of heights\n",
    " * correlate detection performance against tidal phase\n",
    "\n",
    "**Data:** Beyond tidal data, environmental variables have been collected for 3 hour intervals. Water velocity is used from those variables to determine its potential effect on detection performance.\n",
    "\n",
    "Summary plots are presented separately for each receiver / transmitter combination at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import rcParams\n",
    "from pandas_ods_reader import read_ods\n",
    "# mpl.use('module://ipympl.backend_nbagg')\n",
    "import os\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def printmd(strmd):\n",
    "    display(Markdown(strmd))\n",
    "def file_path(*fn, folder=\"data\"):\n",
    "    return os.path.join(\"..\", folder, *fn)\n",
    "\n",
    "#%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "rcParams['font.size'] = 14\n",
    "rcParams[\"legend.framealpha\"] = 0.6\n",
    "rcParams['figure.dpi']= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detections_merged = pd.read_csv(file_path(\"Range_Test_VUE_Export_detections_use.csv\", folder=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_detections_merged[\"datetime\"] = pd.to_datetime(df_detections_merged[\"Date and Time\"],\n",
    "                                                  format=\"%m/%d/%y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection data is merged with environmental variables\n",
    "\n",
    "The current example merges HYCOM environmental data via **pre-processing that is not included in this notebook**.\n",
    "\n",
    "Automated data fetching is one of the TODO items in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: source environmental data from pyERDAP or kadlu.fetch\n",
    "\n",
    "df_detections_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine tidal heights via interpolation of tidal time tables\n",
    "\n",
    "In addition to ocean and weather model data, historic tidal tables are available and used here to provide additional information about environmental cycles that could be factors of influence on the acoustic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dftt = pd.read_excel(file_path(\"Extracted_tidal_times_for_Halifax_2016.xlsx\"), 0)\n",
    "#print(\"Times are in UTC @ Halifax, Heights are in Centimetres @ Halifax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidal_times = read_ods(file_path(\"Extracted_tidal_times_for_Halifax_2016_2sheets.ods\"), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tidal_table(df, year, format_str = \"%d %B %Y %H%M\", display=False):\n",
    "    dflat = None\n",
    "    for cn in range(1,5):\n",
    "        tc = \"time{}\".format(cn)\n",
    "        hc = \"height{}\".format(cn)\n",
    "        timehhmm = df[tc]\n",
    "        heights = df[hc]\n",
    "        if False: # don't interpolate, just drop NaN's\n",
    "            heights = heights.interpolate(\"linear\")\n",
    "            timehhmm = timehhmm.interpolate(\"pad\")\n",
    "        is_miss = df[hc].isnull()\n",
    "        dff = pd.concat([pd.to_datetime(\n",
    "                            df.Day.map(int).map(str) + \" \" + df.Month + \" {} \".format(year) + timehhmm,\n",
    "                            format=format_str).rename(\"time\"),\n",
    "                         heights.rename(\"height\")],\n",
    "                        axis=1).set_index(\"time\")\n",
    "        if dflat is None:\n",
    "            dflat = dff\n",
    "        else:\n",
    "            dflat = dflat.append(dff)\n",
    "\n",
    "    dflat = dflat.dropna().sort_values(by=\"time\")\n",
    "    dflat.loc[:,\"low\"] = list(\"lh\" * (len(dflat)//2))\n",
    "\n",
    "    if display:\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            display(dflat)\n",
    "    return dflat\n",
    "\n",
    "dflat = flatten_tidal_table(df_tidal_times, 2016)\n",
    "dflat.to_csv(\"output_tidal_times_for_Halifax_2016_flat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"\"\"\n",
    "## Tidal data for Halifax\n",
    "Linear interpolation\n",
    "\"\"\")\n",
    "dflat[\"height\"].plot()\n",
    "plt.ylabel(\"height (cm)\")\n",
    "plt.xlabel(None)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflat.loc[:-1,\"duration\"] = dflat.index[1:] - dflat.index[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflat[\"time_start\"] = dflat.index\n",
    "dflat[\"height_start\"] = dflat.height\n",
    "dflat[\"height_change\"] = -dflat.height.diff(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_times = pd.date_range(\"2016-03-07 00:18\", \"2016-04-05 18:23\", freq=\"300s\")\n",
    "new_times = df_detections_merged.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = dflat.index.union(new_times).drop_duplicates().astype(dflat.index.dtype)\n",
    "dfi = dflat.reindex(new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for col in ['low','duration','time_start', 'height_start', 'height_change']:\n",
    "    dfi[col].interpolate(\"pad\", inplace=True)\n",
    "dfi[\"t\"] = (dfi.index - dfi.time_start) / dfi.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi[\"t2\"] = dfi[\"t\"] + (dfi[\"low\"] == \"h\")\n",
    "dfi.height = dfi.height_start + (dfi.height_change * 0.5*(1-np.cos(dfi.t*np.pi)))\n",
    "#dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(dfi), len(-dfi.height.diff(-1)[:-1] / ((dfi.index[1:] - dfi.index[:-1]) / pd.Timedelta(\"1h\")))\n",
    "#dfi[\"dheight_cm_per_hr\"] = dheight_cm_per_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi[\"dheight_cm_per_hr\"] = -dfi.height.diff(-1)[:-1] / ((dfi.index[1:] - dfi.index[:-1]) / pd.Timedelta(\"1h\"))\n",
    "#dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"output_tidal_times_for_Halifax_2016_5min.csv\"\n",
    "dfi.to_csv(fname)\n",
    "printmd(\"Wrote data to `{}`\".format(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end_datetime = \"2016-03-15 01:18\"\n",
    "with plt.rc_context({'figure.figsize': (16, 5), 'lines.linewidth': 2}):\n",
    "    end_datetime = dfi.index.max()\n",
    "    printmd(\"Display data until {}\".format(end_datetime))\n",
    "    dfi.loc[:end_datetime].height.plot()\n",
    "    dfi.loc[:end_datetime].dheight_cm_per_hr.plot()\n",
    "    (dfi.loc[:end_datetime].t*10).plot()\n",
    "    plt.title(\"Tidal data for Halifax with Cosine interpolation\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $t$ above indicate tidal phase within each of high-to-low and low-to-high portion. Its range is in $[0,1]$, but has been magnified by a factor of $10$ in the plot to show more clearly in comparison to the other variables.\n",
    "\n",
    "Below, a new variable $t2$ is introduced that ranges from $0$ to $2$, from high tide to the next high tide, with $1$ corresponding to low tide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmm = df_detections_merged.merge(dfi[[\"t2\",\"height\",\"dheight_cm_per_hr\"]], left_on=\"datetime\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = dfmm.set_index(\"datetime\").sort_index()\n",
    "del dj[\"Date\"], dj[\"Time\"], dj[\"Date and Time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection data is grouped by distinct \"Receiver\", \"Transmitter\" pairs\n",
    "Later, each of these groups is analysed separately.\n",
    "A name is produced for each pairing that reflects their configuration, such as power level, tag family, distance - as determined by parsing the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djg = dj.groupby([\"Receiver\",\"Transmitter\"])\n",
    "groups = [djg.get_group(x) for x in djg.groups]\n",
    "# list(map(len, groups))\n",
    "\n",
    "# groups is a list of DataFrames that have the respective detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receiver / Transmitter metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "def dist_m(latlon0, latlon1):\n",
    "    return geodesic(latlon0, latlon1).m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeta = pd.read_csv(file_path(\"range_test_raw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File `range_test_raw.csv` does not have further metainfo merged in. This will be fixed with additional code, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest metadata, data dictionary, and deployment info\n",
    "\n",
    "* Load data sheets\n",
    "* Correct column names, convert integers, convert datetimes\n",
    "* Merge Recv/Tag info with meta data, calculate geodesic distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = file_path(\"metadata-from-initial-range-test.xls\")\n",
    "sheet_skips = {'Data Dictionary':4, 'Deployment':0}\n",
    "dfmeta_data = dict((sname, pd.read_excel(metadata_file, sheet_name=sname, skiprows=skipr))\n",
    "                   for sname, skipr in sheet_skips.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeta_datadict = dfmeta_data['Data Dictionary'].set_index('Field Name')\n",
    "dfmeta_deploy = dfmeta_data['Deployment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address possibly inconsistent use of _NUMBER vs _NO, by renaming all to _NO\n",
    "dfmeta_datadict = dfmeta_datadict.set_index(dfmeta_datadict.index.str.replace('NUMBER','NO'))\n",
    "dfmeta_deploy.columns = dfmeta_deploy.columns.str.replace('NUMBER','NO')\n",
    "units_col = next(filter(lambda c: 'units' in c.lower(), dfmeta_datadict.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove (format) part from column names in deployment table\n",
    "col_names_split = dfmeta_deploy.columns.str.split(n=1)\n",
    "dfmeta_deploy.columns = col_names_split.str[0]\n",
    "col_formats = col_names_split.str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Format column to data dictionary\n",
    "dfmeta_datadict['Format'] = pd.DataFrame(col_names_split.str).transpose().set_index(0)\n",
    "paren_regex = r'\\((.*)\\)'\n",
    "dfmeta_datadict['Format'] = dfmeta_datadict['Format'].str.extract(paren_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN rows that do not have OTN_ARRAY specified\n",
    "dfmeta_deploy.dropna(subset=['OTN_ARRAY'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine columns that have format: integer ... in data dictionary\n",
    "integer_cols = (dfmeta_datadict.index[dfmeta_datadict[units_col].str.match(r\".*format: (integer.*)\")].tolist()\n",
    "                + ['INS_SERIAL_NO', 'AR_SERIAL_NO'])\n",
    "dfmeta_datadict.loc[integer_cols,'Format'] = 'integer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform type conversion of integer columns, use special int to fill NaNs\n",
    "NANINT = 0\n",
    "dfmeta_deploy[integer_cols] = dfmeta_deploy[integer_cols].replace({np.nan:NANINT}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_otn_datetime_format_str(format_str, split_char = \"T\"):\n",
    "    def replace_all(s, repdict):\n",
    "        for a, b in repdict.items():\n",
    "            s = s.replace(a, b)\n",
    "        return s\n",
    "    date_replacements = {\"yyyy\":\"%Y\", \"mm\":\"%m\", \"dd\":\"%d\"}\n",
    "    time_replacements = {\"hh\":\"%H\", \"mm\":\"%M\", \"ss\":\"%S\"}\n",
    "    date_part, time_part = format_str.split(\"T\")\n",
    "    date_part = replace_all(date_part, date_replacements)\n",
    "    time_part = replace_all(time_part, time_replacements)\n",
    "    return date_part + split_char + time_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dfmeta_datadict.index[dfmeta_datadict.index.str.match('.*DATE_TIME.*')]:\n",
    "    format_str = convert_otn_datetime_format_str(dfmeta_datadict.loc[col, 'Format'])\n",
    "    dfmeta_deploy[col.replace(\"DATE_TIME\", \"DATETIME\")] = pd.to_datetime(dfmeta_deploy[col],\n",
    "                                                                         format=format_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 30,\n",
    "                       'display.max_columns', 100,\n",
    "                       'display.max_colwidth', -1):\n",
    "    display(Markdown(\"### Data dictionary\"))\n",
    "    display(dfmeta_datadict)\n",
    "    display(Markdown(\"### Deployment info\"))\n",
    "    display(dfmeta_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_lat_lon = dfmeta_deploy.groupby('STATION_NO')[['DEPLOY_LAT','DEPLOY_LONG']].nth(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy_lat_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_dists_m = pd.DataFrame(None, columns=deploy_lat_lon.index, index=deploy_lat_lon.index)\n",
    "for stationA in deploy_lat_lon.index:\n",
    "    station_dists_m.loc[stationA, stationA] = 0\n",
    "    for stationB in deploy_lat_lon.loc[stationA+1:,:].index:\n",
    "        dABm = dist_m(deploy_lat_lon.loc[stationA, :].values,\n",
    "                      deploy_lat_lon.loc[stationB, :].values)\n",
    "        station_dists_m.loc[stationA, stationB] = dABm\n",
    "        station_dists_m.loc[stationB, stationA] = dABm\n",
    "display(Markdown(\"\"\"### Station distances\n",
    "* geodesic\n",
    "* in meters\n",
    "* ignoring depth difference\n",
    "\n",
    "Distance between stations 2 and 3 does not occur in detections, since there are no receivers at these stations.\n",
    "\"\"\"))\n",
    "station_dists_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_specs_df = pd.read_excel(file_path(\"tag-specs-Mahone-Bay-range-test\", \"tag-summary-mahone-bay-range-test.xls\"))\n",
    "#tag_specs.columns\n",
    "tag_specs = tag_specs_df[['Tag Family','ID Code','VUE Tag ID\\n(Freq-Space-ID)','Power\\n(L/H)']].copy(deep=False)\n",
    "tag_specs.rename(columns={'VUE Tag ID\\n(Freq-Space-ID)':'VUE Tag',\n",
    "                          'Power\\n(L/H)':'Power'}, inplace=True)\n",
    "tag_specs.set_index(\"VUE Tag\", inplace=True)\n",
    "tag_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"\"\"### Merge tag ID Code with INS_SERIAL_NO to get metadata\n",
    "The tags that have missing info here, turn out to be unimportant later, due to insufficient detection count.\n",
    "\"\"\"))\n",
    "tag_specs.merge(dfmeta_deploy, 'left', left_on='ID Code', right_on='INS_SERIAL_NO').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_id(device_str):\n",
    "    \"Return last part of '-'-separated string as int. Works on str and DataFrames of strings.\"\n",
    "    try:\n",
    "        return device_str.str.split(\"-\").str[-1].astype(int)\n",
    "    except:\n",
    "        return int(device_str.split(\"-\")[-1])\n",
    "\n",
    "def rt_info(grdf):\n",
    "    # get receiver and transmitter IDs of first detection and merge metadata\n",
    "    rt_inf = pd.DataFrame(get_device_id(grdf.iloc[0,:][['Receiver','Transmitter']])).transpose()\n",
    "    rt_inf = rt_inf.merge(dfmeta_deploy.add_prefix('RECV_'), 'left', left_on='Receiver', right_on='RECV_INS_SERIAL_NO')\n",
    "    rt_inf = rt_inf.merge(dfmeta_deploy.add_prefix('TAG_'), 'left', left_on='Transmitter', right_on='TAG_INS_SERIAL_NO')\n",
    "    rt_inf = rt_inf.iloc[0]\n",
    "    try:\n",
    "        rt_inf['RT_DISTANCE_M'] = station_dists_m.loc[rt_inf['RECV_STATION_NO'], rt_inf['TAG_STATION_NO']]\n",
    "    except:\n",
    "        pass\n",
    "    return rt_inf\n",
    "\n",
    "def rt_dist(grdf):\n",
    "    rt_inf = rt_info(grdf)\n",
    "    try:\n",
    "        return rt_inf['RT_DISTANCE_M']\n",
    "    except:\n",
    "        #print(rt_inf)\n",
    "        return np.nan\n",
    "\n",
    "def rt_name(grdf, dist_str=None):\n",
    "    try:\n",
    "        metainf = tag_specs.loc[grdf.iloc[0][\"Transmitter\"], ['Tag Family','Power']]\n",
    "        metainf = \"/\" + \"-\".join(metainf.values)\n",
    "        if dist_str:\n",
    "            metainf += \"-\" + dist_str(rt_dist(grdf))\n",
    "    except:\n",
    "        metainf = ''\n",
    "    rt = grdf.iloc[0][[\"Receiver\",\"Transmitter\"]]\n",
    "    rt[\"Transmitter\"] = \"tag-%d\" % get_device_id(rt[\"Transmitter\"])\n",
    "    dscr = \"/\".join(rt) + metainf\n",
    "    return dscr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec1 = pd.Timedelta(\"1s\")\n",
    "def calc_intervals(grdf):\n",
    "    return -grdf.reset_index().datetime.diff(-1) / sec1\n",
    "def group_info(grdf=None):\n",
    "    if grdf is None:\n",
    "        return \"count\", \"min_interval\", \"max_interval\", \"Receiver/Transmitter\", \"dist_m\"\n",
    "    ivs = calc_intervals(grdf)\n",
    "    return len(grdf), ivs.min(), ivs.max(), rt_name(grdf), rt_dist(grdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsdf = pd.DataFrame((group_info(grdf) for grdf in groups),\n",
    "                    columns=[*group_info()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_min, d_max = gsdf['dist_m'].max(), gsdf['dist_m'].min()\n",
    "dist_th = np.mean((d_min, d_max))\n",
    "def dist_str(dist):\n",
    "    if np.isnan(dist):\n",
    "        return \"U\"\n",
    "    elif dist > dist_th:\n",
    "        return \"F\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "gsdf['Receiver/Transmitter'] = gsdf['Receiver/Transmitter'] + \"-\" + gsdf['dist_m'].apply(dist_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"\"\"# Summary of detections by Receiver/Transmitter pair\n",
    "**R/T name format:**  \n",
    "Receiver/Transmitter/Tag Family/Power(H,L)/Distance(Near,Far)\n",
    "\n",
    "**Distances:**  \n",
    "near = %.2f m  \n",
    "far = %.2f m\n",
    "\"\"\" % (d_min, d_max))\n",
    "gsdf.sort_values(by=\"Receiver/Transmitter\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots of detection density and interval lengths <br/> against tidal phase (t2) and water velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_detection_rate(tdfok, exp_interval_s=300, num_time_bins=200):\n",
    "    \"\"\"calculate detection rate\"\"\"\n",
    "    # shift time stamps to start at 0 for first measurement iloc[0]\n",
    "    time_stamps = (tdfok.datetime - tdfok.datetime.iloc[0]) / sec1\n",
    "    dtcnt, bins_dt = np.histogram(time_stamps, bins=num_time_bins)\n",
    "    expcount = np.ceil((bins_dt[1]-bins_dt[0]) / exp_interval_s) # expected count per bin if no ping is lost\n",
    "    bins_rt = bins_dt[:-1] * sec1 + np.datetime64(tdfok.datetime.iloc[0]) # shift bin timestamps back to actual time\n",
    "    det_rate = pd.Series(data=dtcnt/expcount,  # detected count normalized by expected count (may be > 1)\n",
    "                         index=bins_rt,\n",
    "                         name=\"detection_rate\")\n",
    "    tdfok = pd.concat([tdfok.set_index(\"datetime\"), det_rate], axis=1).interpolate(type=\"pad\")\n",
    "    tdfok.index.name = \"datetime\"\n",
    "    return tdfok.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 16, 5\n",
    "rcParams['font.size'] = 11\n",
    "rcParams['figure.max_open_warning'] = 50\n",
    "\n",
    "t2bin_stepsize = 0.05\n",
    "t2bins = np.arange(0, 2+1e-4, t2bin_stepsize)\n",
    "interval_all = np.zeros(len(t2bins)-1)\n",
    "mean_ping_interval = 300 # sec\n",
    "num_time_bins = 200\n",
    "MIN_DETECTIONS = 100 # skip receiver/transmitter combinations that have less than this number of detections\n",
    "rt_name_dist = lambda gr: rt_name(gr, dist_str)\n",
    "\n",
    "skipmsg = False\n",
    "# each group contains all detections for a particular receiver/transmitter combination\n",
    "for gr in groups:\n",
    "    if len(gr) < MIN_DETECTIONS:\n",
    "        if not skipmsg:\n",
    "            printmd(\"Skipping receiver/transmitter combinations due to insufficient detections:\")\n",
    "            skipmsg = True\n",
    "        printmd(\"{}\".format(rt_name_dist(gr)))\n",
    "        continue\n",
    "        \n",
    "    # add interval length and water velocity calculations to dataframe\n",
    "    tdf = gr.copy(deep=False).reset_index()\n",
    "    tdf[\"interval\"] = calc_intervals(tdf.set_index(\"datetime\"))\n",
    "    tdf[\"water_vel\"] = (tdf.water_u**2+tdf.water_v**2).apply(np.sqrt)\n",
    "    #tdf[\"water_vel\"] = (tdf.water_u_bottom**2+tdf.water_v_bottom**2).apply(np.sqrt)\n",
    "    #tdf[\"water_vel\"] = tdf[\"dheight_cm_per_hr\"]\n",
    "    #colnames = ['salinity_bottom', 'water_temp_bottom', 'water_u_bottom', 'water_v_bottom',\n",
    "    #            'salinity', 'water_temp', 'water_u', 'water_v']\n",
    "    #tdf[\"water_vel\"] = tdf[\"salinity\"]\n",
    "\n",
    "    # ignore date range with short signal intervals at beginning\n",
    "    cutoff_t = tdf.loc[tdf[tdf.interval < 2**8].index.max()+1,\n",
    "                       \"datetime\"]\n",
    "    tdfok = tdf[tdf.datetime>cutoff_t].dropna()\n",
    "    tdfok = tdfok.loc[tdfok[\"interval\"] < 2**13]\n",
    "    tdfok = make_detection_rate(tdfok, exp_interval_s=mean_ping_interval, num_time_bins=num_time_bins)\n",
    "    # TODO add tdfok to list for later pd.concat\n",
    "    \n",
    "    if True:\n",
    "        tdfok.set_index(\"datetime\")[[\"detection_rate\",\"water_vel\"]].plot(grid=True)\n",
    "        plt.title(rt_name_dist(gr), fontsize=24)\n",
    "        plt.xlabel(None)\n",
    "\n",
    "    base_interval = tdfok.interval[tdfok.interval<2**9].mean() # should be 5 min = 300 sec\n",
    "    tdfmean = tdfok.groupby(pd.cut(tdfok[\"t2\"], t2bins)).mean()\n",
    "    tdfcount = tdfok.groupby(pd.cut(tdfok[\"t2\"], t2bins)).count()\n",
    "    tdfcount[\"bins\"] = tdfcount.index.map(lambda i: (i.left+i.right)/2)\n",
    "    interval_all += tdfcount[\"t2\"]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "    fig.suptitle(rt_name_dist(gr))\n",
    "    \n",
    "    # plot over tidal phase t2: interval lengths, detection density, mean water_velocity\n",
    "    ax = tdfok.plot.scatter(\"t2\", \"interval\", alpha=0.3, ax=axs[0])\n",
    "    ax.set_yscale(\"log\", basey=2)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_xlabel(\"tidal phase\")\n",
    "    ax.set_title(\"interval lengths\\ndetection count\\nmean water velocity\", fontsize=12)\n",
    "\n",
    "    #tfivs = (tdfok.datetime.max()-tdfok.datetime.min())/sec1/(len(tdfok)/len(t2bins))/tdfcount[\"t2\"].values\n",
    "    tfivs = tdfcount[\"t2\"].values\n",
    "    axs[0].plot(tdfcount[\"bins\"].tolist(), tfivs, c=\"black\")\n",
    "    #ax = tdfmean.plot(\"t2\", \"water_vel\", alpha=1, ax=axs[0], c=\"darkgreen\", linewidth=1)\n",
    "    ax = tdfmean.plot(\"t2\", \"water_vel\", alpha=1, ax=axs[0], c=\"darkgreen\", linewidth=1)\n",
    "    ax.legend([\"detection count\", \"water velocity\"],\n",
    "              loc=0)\n",
    "    ax.grid()\n",
    "    ax.set_xlim(xmin=0, xmax=2)\n",
    "\n",
    "    # interval lengths over date range\n",
    "    if True:\n",
    "        ax = tdf.set_index(\"datetime\")[\"interval\"].plot(style=\".\", ax=axs[1], alpha=.1)\n",
    "        tdfok.set_index(\"datetime\").water_vel.plot(ax=axs[1], c=\"darkgreen\")\n",
    "        axs[1].plot([cutoff_t]*2, [tdfok.interval.min(), tdfok.interval.max()], c=\"darkorange\", linewidth=4)\n",
    "        ax.set_yscale(\"log\", basey=2)\n",
    "        ax.grid()\n",
    "        ax.legend(['interval (blue dots)', 'water velocity'])\n",
    "        ax.xaxis.label.set_visible(False)\n",
    "    #ax = tdfmean.plot(\"t2\",\"interval\", alpha=1, ax=ax1, c=\"darkgrey\", linewidth=2)\n",
    "    elif False:\n",
    "        ax = tdf.set_index(\"datetime\")[\"interval\"].plot(style=\".\", ax=axs[1], alpha=.1)\n",
    "        tdfok.set_index(\"datetime\")[[\"water_vel\", \"detection_rate\"]].plot(ax=ax)\n",
    "        ax.set_yscale(\"log\", basey=2)\n",
    "        ax.grid()\n",
    "        ax.legend(['interval (blue dots)', 'water velocity', 'detection_rate'])\n",
    "    else:\n",
    "        #tdfok.set_index(\"datetime\")[[\"water_vel\", \"detection_rate\"]].plot(ax=axs[1], grid=True)\n",
    "        # plot detection_rate and water_velocity in two vertically stacked plots\n",
    "        mainax = axs[1]\n",
    "        l,b,w,h = mainax.get_position().bounds\n",
    "        axs[1].axis(\"off\")\n",
    "        ax1 = plt.axes([l, b, w, h*.45])\n",
    "        ax2 = plt.axes([l, b + h/2, w, h*.45])\n",
    "        ax = tdfok.set_index(\"datetime\")[\"water_vel\"].plot(ax=ax1, color=\"darkorange\", grid=True)\n",
    "        ax.xaxis.label.set_visible(False)\n",
    "        ax.legend(loc=2)\n",
    "        ax.set_ylim(ymin=0)\n",
    "        ax = tdfok.set_index(\"datetime\")[\"detection_rate\"].plot(ax=ax2, color=\"gray\", grid=True)\n",
    "        plt.tick_params(\"x\", labelbottom=False, bottom=\"off\")\n",
    "        ax.xaxis.label.set_visible(False)\n",
    "        ax.legend(loc=2)\n",
    "        ax.set_ylim(ymin=0)\n",
    "\n",
    "    # detection density per water_vel density\n",
    "    #division = np.arange(0, 0.25001, 0.25/100)\n",
    "    v_min = tdfok[\"water_vel\"].min()\n",
    "    v_max = tdfok[\"water_vel\"].max()\n",
    "    division = np.arange(v_min, v_max+1e-5, (v_max-v_min)/100)\n",
    "    counts, _ = np.histogram(tdfok[\"water_vel\"], bins=division)\n",
    "    vel_grid = pd.date_range(tdfok[\"datetime\"].min(), tdfok.datetime.max(), freq=\"1h\")\n",
    "    count_denom, _ = np.histogram(tdfok.set_index(\"datetime\").reindex(vel_grid, method=\"nearest\")[\"water_vel\"], bins=division)\n",
    "    nzi = count_denom != 0\n",
    "    counts_norm = counts\n",
    "    counts_norm[nzi] = counts[nzi] / (count_denom[nzi] / sum(count_denom))\n",
    "    counts_norm = counts_norm / sum(counts_norm)\n",
    "    # print(counts)\n",
    "    #tdfok[\"water_vel\"].hist(bins=division)\n",
    "    #plt.bar(x=division[1:], height=counts, width=0.25/100)    #text=ax.text(0,0, \"\", va=\"bottom\", ha=\"left\")\n",
    "    axs[2].scatter(x=division[1:][nzi], y=counts_norm[nzi])    #text=ax.text(0,0, \"\", va=\"bottom\", ha=\"left\")\n",
    "    plt.ylim(ymin=0)\n",
    "    axs[2].grid()\n",
    "    axs[2].set_xlabel(\"water velocity\")\n",
    "    axs[2].set_ylabel(\"detection density\")\n",
    "    axs[2].set_title(rt_name_dist(gr), fontsize=12)\n",
    "    plt.subplots_adjust(wspace=.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "In the above visual summary, the **H-N** combinations, i.e. high-power, near distance, are the ones where water velocity shows the least effect on variations in detection rate (detection density). This confirms expectations and shows promise for the proposed study method. Next steps include:\n",
    "\n",
    "- Continue to work with detection rate (DR) as calculated in a fixed grid of time windows\n",
    "- Compare variations of DR with respect to other environmental variables\n",
    "- Import other environmental variables automatically via data source APIs (ERDDAP, kadlu.fetch)\n",
    "- Determine suitable numerical measure of factor importance in addition to visual analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "The above analysis was performed using [data from OTN](http://members.devel.oceantrack.org/erddap/tabledap/otnunit_aat_detections.html) (provided by Jonathan Pye of OTN), in combination with HYCOM environmental data and tidal data provided by Casey Hilliard (Meridian/Dal), with a synthesized dataset prepared by Matthew Berkowitz (SFU), with project definition and guidance provided by Oliver Kirsebom (Dal) and Ines Hessler (Dal) as part of the [Meridian Network](https://meridian.cs.dal.ca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Steven Bergner (SFU)"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "title": "Acoustic tracking performance analysis"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
